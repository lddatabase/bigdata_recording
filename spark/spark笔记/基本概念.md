sparksubmit用于提交任务，在standalone模式下，executor默认是在sparksubmit中   
整个spark执行的过程：构建DAG---> 划分stage ---> 生成task   
sc默认是操作RDD的一个接口（sparkContext） 
sc中只是描述信息，并不是真正的数据，当触发transformation的时候才会真正读取数据  
val lines = sc.textFile("hdfs://172.168.12.123:9000/data")  ------》 创建一个最原始的RDD  
val words = lines.flatmap(_.split(" "))  ----> 此时并没有真正读取数据 
